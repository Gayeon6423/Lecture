{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456e20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykospacing\n",
    "import soynlp\n",
    "import konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5de19c",
   "metadata": {},
   "source": [
    "# 1. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e76d8f",
   "metadata": {},
   "source": [
    "## (1) 띄어쓰기 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b4abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"사랑을 했다 우리가 만나\n",
    "지우지 못할 추억이 됐다\n",
    "볼만한 멜로드라마\n",
    "괜찮은 결말\n",
    "그거면 됐다 널 사랑했다\n",
    "우리가 만든 러브 시나리오\n",
    "이젠 조명이 꺼지고\n",
    "마지막 페이지를 넘기면\n",
    "조용히 막을 내리죠\n",
    "에이 괜찮지만은 않아 이별을 마주한다는 건\n",
    "오늘이었던 우리의 어제에 더는 내일이 없다는 건\n",
    "아프긴 해도 더 끌었음 상처가 덧나니까 Ye\n",
    "널 사랑했고 사랑 받았으니 난 이걸로 됐어\n",
    "나 살아가면서 가끔씩 떠오를 기억\n",
    "그 안에 네가 있다면 그거면 충분해\n",
    "사랑을 했다 우리가 만나\n",
    "지우지 못할 추억이 됐다\n",
    "볼만한 멜로드라마\n",
    "괜찮은 결말\n",
    "그거면 됐다 널 사랑했다\n",
    "우리가 만든 러브 시나리오\n",
    "이젠 조명이 꺼지고\n",
    "마지막 페이지를 넘기면\n",
    "조용히 막을 내리죠\n",
    "갈비뼈 사이사이가 찌릿찌릿한 느낌\n",
    "나 사랑받고 있음을 알게 해주는 눈빛\n",
    "너에게 참 많이도 배웠다 반쪽을 채웠다\n",
    "과거로 두기엔 너무 소중한 사람이었다\n",
    "나 살아가면서 가끔씩 떠오를 기억\n",
    "그 안에 네가 있다면 그거면 충분해\n",
    "사랑을 했다 우리가 만나\n",
    "지우지 못할 추억이 됐다\n",
    "볼만한 멜로드라마\n",
    "괜찮은 결말\n",
    "그거면 됐다 널 사랑했다\n",
    "네가 벌써 그립지만 그리워하지 않으려 해\n",
    "한 편의 영화 따스했던 봄으로 너를 기억할게\n",
    "우리가 만든 러브 시나리오\n",
    "이젠 조명이 꺼지고\n",
    "마지막 페이지를 넘기면\n",
    "조용히 막을 내리죠\n",
    "우린 아파도 해봤고\n",
    "우습게 질투도 했어\n",
    "미친 듯이 사랑했고\n",
    "우리 이 정도면 됐어\n",
    "사랑을 했다\n",
    "우리가 만나\n",
    "그거면 됐다\n",
    "널 사랑했다 접기\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2089902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str  = my_str.replace('\\n', ' ') # 줄바꿈 문자 제거 줄바꿈 문자(\\n)을 띄어쓰기로 변환\n",
    "my_str2 = my_str.replace(' ','') # 띄어쓰기를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d380e869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼만한 멜로드라마 괜찮은 결말 그거면 됐다 널 사랑했다 우리가 만든 러브 시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 에이 괜찮지만은 않아 이별을 마주한다는 건 오늘이었던 우리의 어제에 더는 내일이 없다는 건 아프긴 해도 더 끌었음 상처가 덧나니까 Ye 널 사랑했고 사랑 받았으니 난 이걸로 됐어 나 살아가면서 가끔씩 떠오를 기억 그 안에 네가 있다면 그거면 충분해 사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼만한 멜로드라마 괜찮은 결말 그거면 됐다 널 사랑했다 우리가 만든 러브 시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 갈비뼈 사이사이가 찌릿찌릿한 느낌 나 사랑받고 있음을 알게 해주는 눈빛 너에게 참 많이도 배웠다 반쪽을 채웠다 과거로 두기엔 너무 소중한 사람이었다 나 살아가면서 가끔씩 떠오를 기억 그 안에 네가 있다면 그거면 충분해 사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼만한 멜로드라마 괜찮은 결말 그거면 됐다 널 사랑했다 네가 벌써 그립지만 그리워하지 않으려 해 한 편의 영화 따스했던 봄으로 너를 기억할게 우리가 만든 러브 시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 우린 아파도 해봤고 우습게 질투도 했어 미친 듯이 사랑했고 우리 이 정도면 됐어 사랑을 했다 우리가 만나 그거면 됐다 널 사랑했다 접기\n"
     ]
    }
   ],
   "source": [
    "print(my_str) # 줄바꿈문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c079f7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑을했다우리가만나지우지못할추억이됐다볼만한멜로드라마괜찮은결말그거면됐다널사랑했다우리가만든러브시나리오이젠조명이꺼지고마지막페이지를넘기면조용히막을내리죠에이괜찮지만은않아이별을마주한다는건오늘이었던우리의어제에더는내일이없다는건아프긴해도더끌었음상처가덧나니까Ye널사랑했고사랑받았으니난이걸로됐어나살아가면서가끔씩떠오를기억그안에네가있다면그거면충분해사랑을했다우리가만나지우지못할추억이됐다볼만한멜로드라마괜찮은결말그거면됐다널사랑했다우리가만든러브시나리오이젠조명이꺼지고마지막페이지를넘기면조용히막을내리죠갈비뼈사이사이가찌릿찌릿한느낌나사랑받고있음을알게해주는눈빛너에게참많이도배웠다반쪽을채웠다과거로두기엔너무소중한사람이었다나살아가면서가끔씩떠오를기억그안에네가있다면그거면충분해사랑을했다우리가만나지우지못할추억이됐다볼만한멜로드라마괜찮은결말그거면됐다널사랑했다네가벌써그립지만그리워하지않으려해한편의영화따스했던봄으로너를기억할게우리가만든러브시나리오이젠조명이꺼지고마지막페이지를넘기면조용히막을내리죠우린아파도해봤고우습게질투도했어미친듯이사랑했고우리이정도면됐어사랑을했다우리가만나그거면됐다널사랑했다접기\n"
     ]
    }
   ],
   "source": [
    "print(my_str2) # 억지로 띄어쓰기 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a598206",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacer = pykospacing.Spacing()\n",
    "result = spacer(my_str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696aad65",
   "metadata": {},
   "source": [
    "## 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9078e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼만한 멜로드라마 괜찮은 결말 그거면 됐다 널 사랑했다 우리가 만든 러브 시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 에이 괜찮지만은 않아 이별을 마주한다는 건 오늘이었던 우리의 어제에 더는 내일이 없다는 건 아프긴 해도 더 끌었음 상처가 덧나니까 Ye 널 사랑했고 사랑 받았으니 난 이걸로 됐어 나 살아가면서 가끔씩 떠오를 기억 그 안에 네가 있다면 그거면 충분해 사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼만한 멜로드라마 괜찮은 결말 그거면 됐다 널 사랑했다 우리가 만든 러브 시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 갈비뼈 사이사이가 찌릿찌릿한 느낌 나 사랑받고 있음을 알게 해주는 눈빛 너에게 참 많이도 배웠다 반쪽을 채웠다 과거로 두기엔 너무 소중한 사람이었다 나 살아가면서 가끔씩 떠오를 기억 그 안에 네가 있다면 그거면 충분해 사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼만한 멜로드라마 괜찮은 결말 그거면 됐다 널 사랑했다 네가 벌써 그립지만 그리워하지 않으려 해 한 편의 영화 따스했던 봄으로 너를 기억할게 우리가 만든 러브 시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 우린 아파도 해봤고 우습게 질투도 했어 미친 듯이 사랑했고 우리 이 정도면 됐어 사랑을 했다 우리가 만나 그거면 됐다 널 사랑했다 접기\n"
     ]
    }
   ],
   "source": [
    "## 원본\n",
    "print(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d450c00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼 만한 멜로 드라마 괜찮은 결말 그거면 됐다 널 사랑했다 우리가 만든 러브시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠에 이 괜찮지만은 않아 이별을 마주한다는 건 오늘이었던 우리의 어제에 더는 내일이 없다는 건 아프긴 해도 더 끌었음 상처가 덧나니까 Ye널 사랑했고 사랑받았으니 난 이걸로 됐어나 살아가면서 가끔씩 떠오를 기억 그 안에 네가 있다면 그거면 충분해 사랑을 했다 우리가 만나 지우지 못할 추억 이 됐다 볼 만한 멜로 드라마 괜찮은 결말 그거면 됐다 널 사랑했다 우리가 만든 러브시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 갈비뼈 사이사이가 찌릿찌릿한 느낌나 사랑받고 있음을 알게 해주는 눈빛 너에게 참 많이 도 배웠다 반쪽을 채웠다 과거로 두기엔 너무 소중한 사람이었다 나 살아가면서 가끔씩 떠오를 기억 그 안에 네가 있다면 그거면 충분해 사랑을 했다 우리가 만나 지우지 못할 추억이 됐다 볼 만한 멜로 드라마 괜찮은 결말 그거면 됐다 널 사랑했다네가 벌써 그립지만 그리워하지 않으려 해 한 편의 영화 따스했던 봄으로 너를 기억할 게 우리가 만든 러브시나리오 이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠 우린 아파도 해봤고 우습게 질투도 했어 미친 듯이 사랑했고 우리 이 정도면 됐어 사랑을 했다 우리가 만나 그거면 됐다 널 사랑했다 접기\n"
     ]
    }
   ],
   "source": [
    "## 결과\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bb9b2",
   "metadata": {},
   "source": [
    "## (2) 맞춤법 교정\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Py-Hanspell이 정상적으로 작동하지 않습니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>업데이트 한게 꽤 오래전 일이라...</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>카카오, 네이버 API를 사용해야 합니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>혹은 아래 페이지를 참고하여 부산대 한국어 맞춤법 검사기를 파이썬에서 사용하는 코드를 작성해서 사용하세요.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>https://hleecaster.com/speller-cs-pusan-ac-kr/</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17565763",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821838f9",
   "metadata": {},
   "source": [
    "## (1) Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3990dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c681bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"사랑을 했다. 우리가 만나.\n",
    "지우지 못할 추억이 됐다.\n",
    "볼만한 멜로드라마,\n",
    "괜찮은 결말,\n",
    "그거면 됐다. 널 사랑했다.\n",
    "우리가 만든 러브 시나리오.\n",
    "이젠 조명이 꺼지고\n",
    "마지막 페이지를 넘기면\n",
    "조용히 막을 내리죠.\"\"\"\n",
    "\n",
    "my_str = my_str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19820bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['사랑을 했다.',\n",
       " '우리가 만나.',\n",
       " '지우지 못할 추억이 됐다.',\n",
       " '볼만한 멜로드라마, 괜찮은 결말, 그거면 됐다.',\n",
       " '널 사랑했다.',\n",
       " '우리가 만든 러브 시나리오.',\n",
       " '이젠 조명이 꺼지고 마지막 페이지를 넘기면 조용히 막을 내리죠.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kss.split_sentences(my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9be9a",
   "metadata": {},
   "source": [
    "## (2) Word Tokenization \n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>한국어에서 word tokenization은 형태소 분석과 거의 동일한 행위</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce54eee",
   "metadata": {},
   "source": [
    "### (1) konlpy의 Okt(Open Korea Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09949d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df1c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"사랑을 했다 우리가 만나\n",
    "지우지 못할 추억이 됐다\n",
    "볼만한 멜로드라마\n",
    "괜찮은 결말\n",
    "그거면 됐다 널 사랑했다\n",
    "우리가 만든 러브 시나리오\n",
    "이젠 조명이 꺼지고\n",
    "마지막 페이지를 넘기면\n",
    "조용히 막을 내리죠\"\"\"\n",
    "\n",
    "my_str = my_str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7469791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adjective': '형용사',\n",
       " 'Adverb': '부사',\n",
       " 'Alpha': '알파벳',\n",
       " 'Conjunction': '접속사',\n",
       " 'Determiner': '관형사',\n",
       " 'Eomi': '어미',\n",
       " 'Exclamation': '감탄사',\n",
       " 'Foreign': '외국어, 한자 및 기타기호',\n",
       " 'Hashtag': '트위터 해쉬태그',\n",
       " 'Josa': '조사',\n",
       " 'KoreanParticle': '(ex: ㅋㅋ)',\n",
       " 'Noun': '명사',\n",
       " 'Number': '숫자',\n",
       " 'PreEomi': '선어말어미',\n",
       " 'Punctuation': '구두점',\n",
       " 'ScreenName': '트위터 아이디',\n",
       " 'Suffix': '접미사',\n",
       " 'Unknown': '미등록어',\n",
       " 'Verb': '동사'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b209f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석:  ['사랑', '을', '했다', '우리', '가', '만나', '지우지', '못', '할', '추억', '이', '됐다', '볼', '만', '한', '멜로드라마', '괜찮은', '결말', '그거', '면', '됐다', '널', '사랑', '했다', '우리', '가', '만든', '러브', '시나리오', '이', '젠', '조명', '이', '꺼지고', '마지막', '페이지', '를', '넘기면', '조용히', '막', '을', '내리죠']\n"
     ]
    }
   ],
   "source": [
    "# tokenize(형태소 분석)\n",
    "print('OKT 형태소 분석: ', okt.morphs(my_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add66f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 품사 태깅:  [('사랑', 'Noun'), ('을', 'Josa'), ('했다', 'Verb'), ('우리', 'Noun'), ('가', 'Josa'), ('만나', 'Verb'), ('지우지', 'Verb'), ('못', 'VerbPrefix'), ('할', 'Verb'), ('추억', 'Noun'), ('이', 'Josa'), ('됐다', 'Verb'), ('볼', 'Noun'), ('만', 'Suffix'), ('한', 'Josa'), ('멜로드라마', 'Noun'), ('괜찮은', 'Adjective'), ('결말', 'Noun'), ('그거', 'Noun'), ('면', 'Josa'), ('됐다', 'Verb'), ('널', 'Noun'), ('사랑', 'Noun'), ('했다', 'Verb'), ('우리', 'Noun'), ('가', 'Josa'), ('만든', 'Verb'), ('러브', 'Noun'), ('시나리오', 'Noun'), ('이', 'Determiner'), ('젠', 'Noun'), ('조명', 'Noun'), ('이', 'Josa'), ('꺼지고', 'Verb'), ('마지막', 'Noun'), ('페이지', 'Noun'), ('를', 'Josa'), ('넘기면', 'Verb'), ('조용히', 'Adjective'), ('막', 'Noun'), ('을', 'Josa'), ('내리죠', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "print('OKT 품사 태깅: ', okt.pos(my_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92d46303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 명사 추출:  ['사랑', '우리', '추억', '볼', '멜로드라마', '결말', '그거', '널', '사랑', '우리', '러브', '시나리오', '젠', '조명', '마지막', '페이지', '막']\n"
     ]
    }
   ],
   "source": [
    "# Noun Extraction\n",
    "print('OKT 명사 추출: ', okt.nouns(my_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7ac43",
   "metadata": {},
   "source": [
    "### (2) konlpy의 Kkma (꼬꼬마)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6c551b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b7c336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kkma 형태소 분석:  ['사랑', '을', '하', '었', '다', '우리', '가', '만', '나', '지우', '지', '못하', 'ㄹ', '추억', '이', '되', '었', '다', '볼만', '하', 'ㄴ', '멜로드라마', '괜찮', '은', '결말', '그거', '이', '면', '되', '었', '다', '넣', 'ㄹ', '사랑', '하', '었', '다', '우리', '가', '만들', 'ㄴ', '러브', '시나리오', '이젠', '조명', '이', '꺼지', '고', '마지막', '페이지', '를', '넘기', '면', '조용히', '막', '을', '내리', '죠']\n"
     ]
    }
   ],
   "source": [
    "print('Kkma 형태소 분석: ', kkma.morphs(my_str, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3726bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kkma 품사 태깅:  [('사랑', 'NNG'), ('을', 'JKO'), ('하', 'VV'), ('었', 'EPT'), ('다', 'ECS'), ('우리', 'NP'), ('가', 'JKS'), ('만', 'JX'), ('나', 'NP'), ('지우', 'VV'), ('지', 'ECD'), ('못하', 'VX'), ('ㄹ', 'ETD'), ('추억', 'NNG'), ('이', 'JKC'), ('되', 'VV'), ('었', 'EPT'), ('다', 'ECS'), ('볼만', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('멜로드라마', 'NNG'), ('괜찮', 'VA'), ('은', 'ETD'), ('결말', 'NNG'), ('그거', 'NP'), ('이', 'VCP'), ('면', 'ECE'), ('되', 'VV'), ('었', 'EPT'), ('다', 'ECS'), ('넣', 'VV'), ('ㄹ', 'ETD'), ('사랑', 'NNG'), ('하', 'XSV'), ('었', 'EPT'), ('다', 'ECS'), ('우리', 'NP'), ('가', 'JKS'), ('만들', 'VV'), ('ㄴ', 'ETD'), ('러브', 'NNG'), ('시나리오', 'NNG'), ('이젠', 'NNP'), ('조명', 'NNG'), ('이', 'JKS'), ('꺼지', 'VV'), ('고', 'ECE'), ('마지막', 'NNG'), ('페이지', 'NNG'), ('를', 'JKO'), ('넘기', 'VV'), ('면', 'ECE'), ('조용히', 'MAG'), ('막', 'NNG'), ('을', 'JKO'), ('내리', 'VV'), ('죠', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print('Kkma 품사 태깅: ', kkma.pos(my_str, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dec69cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kkma 명사 추출:  ['사랑', '우리', '나', '추억', '볼만', '멜로', '멜로드라마', '드라마', '결말', '그거', '러브', '시나리오', '이젠', '조명', '마지막', '페이지', '막']\n"
     ]
    }
   ],
   "source": [
    "print('Kkma 명사 추출: ', kkma.nouns(my_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146d832",
   "metadata": {},
   "source": [
    "# 3. Lemmatization\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Konlpy에서 lemmatize기능은 별도로 존재하지 않습니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>필요하다면 별도로 코드를 작성하셔야합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>이 코드는 다음 웹사이트를 참조하였습니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>참고: https://lovit.github.io/nlp/2019/01/22/trained_kor_lemmatizer/</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b307f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77dc72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "komoran 품사 태깅  [('사랑', 'NNG'), ('을', 'JKO'), ('하', 'VV'), ('았', 'EP'), ('다', 'EC'), ('우리', 'NP'), ('가', 'JKS'), ('만나', 'VV'), ('아', 'EC'), ('지우', 'VV'), ('지', 'EC'), ('못하', 'VX'), ('ㄹ', 'ETM'), ('추억', 'NNG'), ('이', 'JKS'), ('되', 'VV'), ('었', 'EP'), ('다', 'EC'), ('보', 'VV'), ('ㄹ', 'ETM'), ('만', 'NNB'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('멜로드라마', 'NNP'), ('괜찮', 'VA'), ('은', 'ETM'), ('결말', 'NNG'), ('그것', 'NP'), ('이', 'VCP'), ('면', 'EC'), ('되', 'VV'), ('었', 'EP'), ('다', 'EC'), ('너', 'NP'), ('ㄹ', 'JKO'), ('사랑', 'NNG'), ('하', 'XSV'), ('았', 'EP'), ('다', 'EC'), ('우리', 'NP'), ('가', 'JKS'), ('만들', 'VV'), ('ㄴ', 'ETM'), ('러브', 'NNP'), ('시나리오', 'NNP'), ('이제', 'NNG'), ('ㄴ', 'JX'), ('조명', 'NNP'), ('이', 'JKS'), ('꺼지', 'VV'), ('고', 'EC'), ('마지막', 'NNG'), ('페이지', 'NNG'), ('를', 'JKO'), ('넘기', 'VV'), ('면', 'EC'), ('조용히', 'MAG'), ('막', 'VV'), ('을', 'ETM'), ('내리', 'VV'), ('죠', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "pos_tag = komoran.pos(my_str)\n",
    "print('komoran 품사 태깅 ', pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10870c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 참고: https://lovit.github.io/nlp/2019/01/22/trained_kor_lemmatizer/\n",
    "def lemmatize(tag):\n",
    "    if tag[1] == 'VA' or tag[1] == 'VV':\n",
    "        return tag[0] + '다'\n",
    "    else:\n",
    "        return tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95080add",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑\n",
      "을\n",
      "하다\n",
      "우리\n",
      "가\n",
      "만나다\n",
      "지우다\n",
      "못하\n",
      "추억\n",
      "이\n",
      "되다\n",
      "보다\n",
      "만\n",
      "하\n",
      "멜로드라마\n",
      "괜찮다\n",
      "결말\n",
      "그것\n",
      "이\n",
      "되다\n",
      "너\n",
      "ㄹ\n",
      "사랑\n",
      "하\n",
      "우리\n",
      "가\n",
      "만들다\n",
      "러브\n",
      "시나리오\n",
      "이제\n",
      "조명\n",
      "이\n",
      "꺼지다\n",
      "마지막\n",
      "페이지\n",
      "를\n",
      "넘기다\n",
      "조용히\n",
      "막다\n",
      "내리다\n"
     ]
    }
   ],
   "source": [
    "# 완벽하지는 않지만\n",
    "for ele in pos_tag:\n",
    "    if ele[1] in ['EP','EC','ETM','JX','JK']:\n",
    "        continue\n",
    "    print(lemmatize(ele))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181e1cf",
   "metadata": {},
   "source": [
    "# 4. Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911c364",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Konlpy에서 미리 정의된 stopwords들은 별도로 존재하지 않습니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>사용자가 미리 stopwords들을 정의하고 이를 제외해서 해볼게요.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>https://www.ranks.nl/stopwords/korean</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b90c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"사랑을 했다 우리가 만나\n",
    "지우지 못할 추억이 됐다\n",
    "볼만한 멜로드라마\n",
    "괜찮은 결말\n",
    "그거면 됐다 널 사랑했다\n",
    "우리가 만든 러브 시나리오\n",
    "이젠 조명이 꺼지고\n",
    "마지막 페이지를 넘기면\n",
    "조용히 막을 내리죠\"\"\"\n",
    "my_str = my_str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98a75ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstopwords.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      3\u001b[0m stopwords\u001b[38;5;241m=\u001b[39m[x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m stopwords]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\textdata\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords.txt'"
     ]
    }
   ],
   "source": [
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords=[x.replace('\\n','') for x in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = okt.morphs(my_str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [x for x in tokens if x not in stopwords]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b8dfc",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>그러나 여기서 사용한 stopwords는 하나의 예시일 뿐, 사용자가 정하기 나름입니다!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03a703",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92683bd0",
   "metadata": {},
   "source": [
    "## 1. customized konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0e57f",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>다음과 같은 text를 생각해봅시다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0530a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"지금 무슨 노래 들으세요? 뉴진스의 하입보이요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04eed8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f2b57e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('지금', 'Noun'),\n",
       " ('무슨', 'Noun'),\n",
       " ('노래', 'Noun'),\n",
       " ('들으세요', 'Verb'),\n",
       " ('?', 'Punctuation'),\n",
       " ('뉴진스', 'Noun'),\n",
       " ('의', 'Josa'),\n",
       " ('하', 'Exclamation'),\n",
       " ('입', 'Noun'),\n",
       " ('보이', 'Noun'),\n",
       " ('요', 'Josa'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos(my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ddc132",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>ckonlpy는 예전 konlpy를 기준으로 만들어졌습니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>예전 konlpy의 Twitter 형태소 분석기가 지금 버전에서는 Okt입니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "458ef958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8a4b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\text\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff54b4",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>이전 버전이지만 cKoNLPy에 사전을 추가하는 방식으로 분석해봅시다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de107736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ckonlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mckonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Twitter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ckonlpy'"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a3cd765",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Twitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m twitter \u001b[38;5;241m=\u001b[39m \u001b[43mTwitter\u001b[49m(use_twitter_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m twitter\u001b[38;5;241m.\u001b[39madd_dictionary([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m뉴진스\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m하입보이\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoun\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Twitter' is not defined"
     ]
    }
   ],
   "source": [
    "twitter = Twitter(use_twitter_dictionary=True)\n",
    "twitter.add_dictionary(['뉴진스','하입보이'], 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33884f05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtwitter\u001b[49m\u001b[38;5;241m.\u001b[39mpos(my_str)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twitter' is not defined"
     ]
    }
   ],
   "source": [
    "twitter.pos(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textdata",
   "language": "python",
   "name": "textdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
